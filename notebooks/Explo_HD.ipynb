{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining maching learning\n",
    "You can find in this notebook the exploratory data analysis done over the heart disease dataset, coupled with the simple machine learning algorithm used to predict the diseases.\n",
    "\n",
    "## Plan\n",
    "1. Introduction\n",
    "  1. What are we going to explain here\n",
    "    1. Explain AI/ML\n",
    "    2. Not a tutorial, un exemple concret tout public de à quoi ça pourrait servir\n",
    "    3. Essayer d'expliquer le taff d'un DS pour les gens qu'on connait et ceux interessés en general\n",
    "  2. Why do we need to talk about AI in general\n",
    "    1. Future\n",
    "    2. Generic term needs to be addressed\n",
    "    3. See the good side\n",
    "2. What is a Data Scientist\n",
    "  1. Work with data\n",
    "  2. Data Analysis\n",
    "  2. create models\n",
    "  3. communicate\n",
    "3. Example dataset\n",
    "  1. Explaining the dataset and the goal\n",
    "  2. A few statistics/plots\n",
    "  3. Predicting the heart disease\n",
    "  4. Explaining results\n",
    "4. Communicating results to business/boss\n",
    "  1. Presenting the example's results\n",
    "  2. Another level of abstraction ?\n",
    "5. (OPT) Cleaning codebase/Explaining why we would need another language\n",
    "6. Conclusions, pointing to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics\n",
    "import torch\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import SequentialSampler, DataLoader, WeightedRandomSampler, BatchSampler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset fields description\n",
    "1. age: age in years\n",
    "2. sex: sex (1 = male; 0 = female)\n",
    "3. cp: chest pain type\n",
    "    - Value 1: typical angina\n",
    "    - Value 2: atypical angina\n",
    "    - Value 3: non-anginal pain\n",
    "    - Value 4: asymptomatic\n",
    "4. trestbps: resting blood pressure (in mm Hg on admission to the \n",
    "    hospital)\n",
    "5. chol: serum cholestoral in mg/dl\n",
    "6. fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n",
    "7. restecg: resting electrocardiographic results\n",
    "    - Value 0: normal\n",
    "    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "    - Value 1: upsloping\n",
    "    - Value 2: flat\n",
    "    - Value 3: downsloping\n",
    "12. ca: number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "14. num: diagnosis of heart disease (angiographic disease status)\n",
    "    - Value 0: < 50% diameter narrowing\n",
    "    - Value 1: > 50% diameter narrowing\n",
    "    (in any major vessel: attributes 59 through 68 are vessels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(\"../data/heart-disease/processed.cleveland.data\", delimiter=\",\",\n",
    "            names=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "                    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"])\n",
    "heart_df = heart_df.rename(columns={\"cp\":\"chest_pain\",\n",
    "                         \"thalach\":\"max_heart_rate\",\n",
    "                         \"oldpeak\":\"st_dep_induced\",\n",
    "                         \"ca\":\"num_maj_ves\"})\n",
    "\n",
    "heart_df['num_bin'] = heart_df['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "heart_df = heart_df.replace('?', np.nan)\n",
    "heart_df = heart_df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "heart_df_noncat = heart_df.copy()\n",
    "heart_df[[\"sex\", \"chest_pain\", \"fbs\",\n",
    "          \"restecg\", \"exang\", \"slope\",\n",
    "          \"num_maj_ves\", \"thal\", \"num\", \"num_bin\"]] = heart_df[[\"sex\", \"chest_pain\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"num_maj_ves\", \"thal\", \"num\", \"num_bin\"]].apply(lambda x:x.astype('category'))\n",
    "\n",
    "heart_df_noncat[['num_maj_ves', 'thal']] = heart_df_noncat[['num_maj_ves', 'thal']].astype('float')\n",
    "heart_df_noncat = heart_df_noncat.reset_index(drop=True)\n",
    "\n",
    "X = heart_df_noncat.drop(['num', 'num_bin'], axis=1)\n",
    "X = pd.DataFrame(normalize(X, norm=\"l2\", axis=0), columns=list(X))\n",
    "y = heart_df_noncat['num_bin']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Training set : {}\".format(X_train.shape[0]))\n",
    "print(\"Testing set : {}\\n\".format(X_test.shape[0]))\n",
    "\n",
    "heart_df_noncat = pd.concat([X, y], axis=1, join='inner')\n",
    "\n",
    "heart_df_noncat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=8, ncols=2, figsize=(10, 25))\n",
    "ax = ax.reshape(-1)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.5)\n",
    "for i, col in enumerate(heart_df.select_dtypes(exclude=\"category\").columns):\n",
    "    sns.kdeplot(heart_df[col], ax=ax[i])\n",
    "    ax[i].set_xlabel(col)\n",
    "for i, col in enumerate(heart_df.select_dtypes(include=\"category\").columns):\n",
    "    i += 5\n",
    "    sns.countplot(x=col, data=heart_df, ax=ax[i])\n",
    "    ax[i].set_xlabel(col)\n",
    "    ax[i].set_ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(heart_df_noncat, alpha = 0.3, figsize = (15,15), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlation = heart_df_noncat.corr(min_periods=10)\n",
    "plt.figure(figsize=(15, 13))\n",
    "heatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL CELL FOR RFC START\n",
    "\n",
    "# The goal is to predict if a person has a heart disease or not.\n",
    "# Thus we have decided to try a random forest classifier\n",
    "# Has we want to do a kind of diagnosis thus we want to optimize the recall\n",
    "# Because it is more dangerous to miss a heart disease than to over diagnosis\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [5, 10, 20, 30], 'max_features':[3,4,5,6, None], 'max_depth': [4,5,6,7, None]}\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv=5)\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "best_clf = grid_fit.best_estimator_\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "print(\"Best parameters : \", grid_fit.best_params_)\n",
    "# accuracy = tp + tn / total, pas bon si desequilibre des labels \n",
    "# (le classifier peut donner toujours le même label)\n",
    "print(\"\\nFinal accuracy score on the testing data: {:.4f}\".format(metrics.accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(metrics.fbeta_score(y_test, best_predictions, beta = 0.5,  average=\"micro\")))\n",
    "# recall = tp / tp + fn (total de vrai malade du test set) => très important ici,\n",
    "# car on veut trouver le plus de malade possible (plus grave de rater un malade, que d'en diagnostiquer trop)\n",
    "print(\"recall :\", metrics.recall_score(y_test, best_predictions))\n",
    "# precision = tp / tp + fp (total de malade que le classifier a trouvé) => moins important ici,\n",
    "# car determine combien de diagnostiqués malades, sont vraiment malades \n",
    "print(\"precision :\", metrics.precision_score(y_test, best_predictions))\n",
    "# f1 score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"f1 score :\", metrics.f1_score(y_test, best_predictions))\n",
    "\n",
    "### 3 other cells draft ! to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "for i, feat in enumerate(clf.feature_importances_):\n",
    "    print(\"{} : {}\".format(list(X)[i], feat))\n",
    "    \n",
    "print(\"\\naccuracy :\", clf.score(X_test, y_test))\n",
    "\n",
    "predictions_test = clf.predict(X_test)\n",
    "# accuracy = tp + tn / total, pas bon si desequilibre des labels \n",
    "# (le classifier peut donner toujours le même label)\n",
    "print(\"\\naccuracy :\", metrics.accuracy_score(y_test, predictions_test))\n",
    "# recall = tp / tp + fn (total de vrai malade du test set) => très important ici,\n",
    "# car on veut trouver le plus de malade possible (plus grave de rater un malade, que d'en diagnostiquer trop)\n",
    "print(\"recall :\", metrics.recall_score(y_test, predictions_test))\n",
    "# precision = tp / tp + fp (total de malade que le classifier a trouvé) => moins important ici,\n",
    "# car determine combien de diagnostiqués malades, sont vraiment malades \n",
    "print(\"precision :\", metrics.precision_score(y_test, predictions_test))\n",
    "# f1 score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"f1 score :\", metrics.f1_score(y_test, predictions_test))\n",
    "# Confusion Matrix : 0 negatif, 1 positif => [[tn, fp], [fn, tp]]\n",
    "print(\"\\nconfusion matrix :\\n\", metrics.confusion_matrix(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "parameters = {'n_estimators': [5, 10, 20, 30], 'max_features':[3,4,5,6, None], 'max_depth': [4,5,6,7, None]}\n",
    "\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# TODO: Perform grid search on the claszsifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv=5)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"Accuracy score on testing data: {:.4f}\".format(metrics.accuracy_score(y_test, predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(metrics.fbeta_score(y_test, predictions, beta = 0.5, average=\"micro\")))\n",
    "print(\"recall :\", metrics.recall_score(y_test, predictions))\n",
    "print(\"precision :\", metrics.precision_score(y_test, predictions))\n",
    "print(\"f1 score :\", metrics.f1_score(y_test, predictions))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(grid_fit.best_params_)\n",
    "print(\"\\nFinal accuracy score on the testing data: {:.4f}\".format(metrics.accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(metrics.fbeta_score(y_test, best_predictions, beta = 0.5,  average=\"micro\")))\n",
    "print(\"recall :\", metrics.recall_score(y_test, best_predictions))\n",
    "print(\"precision :\", metrics.precision_score(y_test, best_predictions))\n",
    "print(\"f1 score :\", metrics.f1_score(y_test, best_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Model (can also use single decision tree)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=10)\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "# Extract single tree\n",
    "estimator = model.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = list(X_train),\n",
    "                class_names = iris.target_names,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.heart_df = df\n",
    "        self.number_samples = len(df)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.number_samples\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.heart_df.iloc[idx.item(), :-2]\n",
    "        binary_class = self.heart_df.iloc[idx.item(), -1]\n",
    "        return FloatTensor(features.values), binary_class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    features, labels = list(zip(*samples))\n",
    "    features, labels = torch.cat(features, 0), torch.cat(labels, 0)\n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HeartDataset(heart_df_noncat)\n",
    "weight_one = 1 - sum(heart_df_noncat.iloc[:,-1]) / len(heart_df_noncat)\n",
    "weight = [weight_one if c == 1 else 1 - weight_one for c in heart_df_noncat.iloc[:,-1] ]\n",
    "print(len(dataset))\n",
    "sampler = WeightedRandomSampler(weight, len(dataset))\n",
    "b_sampler = BatchSampler(sampler, 2, True)\n",
    "dataset_loader = DataLoader(dataset=dataset,  num_workers=0, batch_sampler=b_sampler )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(100):\n",
    "    a = [sum(c).numpy() for feat, c in dataset_loader]\n",
    "    temp.append(sum(a)/(len(a)*8))\n",
    "print(\"percent of True in the training set:\", sum(temp)/(len(temp)))\n",
    "\n",
    "#a = [sum(i[1]) for i in validation_set_loader]\n",
    "\n",
    "#print(\"percent of True in the validation set:\", sum(a)/sum([len(i[1]) for i in validation_set_loader]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat, c in dataset_loader:\n",
    "    print(feat.shape, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(13,100), torch.nn.SELU(), torch.nn.Linear(100,50), torch.nn.SELU(), torch.nn.Linear(50,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset_loader, nb_epochs, eta):\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0\n",
    "        for feat, c in dataset_loader:\n",
    "            output = model(feat)\n",
    "            loss = criterion(output, c)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "        print(\"epoch {} : {}\".format(e,sum_loss))\n",
    "\n",
    "def compute_nb_errors(model, dataset_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    nb_data_errors = 0\n",
    "    \n",
    "    for feat, c in dataset_loader:\n",
    "        output = model(feat)\n",
    "        print(output)\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        nb_data_errors += sum(abs(predicted_classes - c)).item()\n",
    "    print(nb_data_errors)\n",
    "    print(len(dataset_loader))\n",
    "\n",
    "\n",
    "    return (1 - nb_data_errors/ (len(dataset_loader)*8)) *100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, dataset_loader, 20, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_nb_errors(model, dataset_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
