{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining maching learning\n",
    "You can find in this notebook the exploratory data analysis done over the heart disease dataset, coupled with the simple machine learning algorithm used to predict the diseases.\n",
    "\n",
    "## Plan\n",
    "1. Introduction\n",
    "  1. What are we going to explain here\n",
    "    1. Explain AI/ML\n",
    "    2. Not a tutorial, un exemple concret tout public de à quoi ça pourrait servir\n",
    "    3. Essayer d'expliquer le taff d'un DS pour les gens qu'on connait et ceux interessés en general\n",
    "  2. Why do we need to talk about AI in general\n",
    "    1. Future\n",
    "    2. Generic term needs to be addressed\n",
    "    3. See the good side\n",
    "2. What is a Data Scientist\n",
    "  1. Work with data\n",
    "  2. Data Analysis\n",
    "  2. create models\n",
    "  3. communicate\n",
    "3. Example dataset\n",
    "  1. Explaining the dataset and the goal\n",
    "  2. A few statistics/plots\n",
    "  3. Predicting the heart disease\n",
    "  4. Explaining results\n",
    "4. Communicating results to business/boss\n",
    "  1. Presenting the example's results\n",
    "  2. Another level of abstraction ?\n",
    "5. (OPT) Cleaning codebase/Explaining why we would need another language\n",
    "6. Conclusions, pointing to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import SequentialSampler, DataLoader, WeightedRandomSampler, BatchSampler\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset fields description\n",
    "1. age: age in years\n",
    "2. sex: sex (1 = male; 0 = female)\n",
    "3. cp: chest pain type\n",
    "    - Value 1: typical angina\n",
    "    - Value 2: atypical angina\n",
    "    - Value 3: non-anginal pain\n",
    "    - Value 4: asymptomatic\n",
    "4. trestbps: resting blood pressure (in mm Hg on admission to the \n",
    "    hospital)\n",
    "5. chol: serum cholestoral in mg/dl\n",
    "6. fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n",
    "7. restecg: resting electrocardiographic results\n",
    "    - Value 0: normal\n",
    "    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "    - Value 1: upsloping\n",
    "    - Value 2: flat\n",
    "    - Value 3: downsloping\n",
    "12. ca: number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "14. num: diagnosis of heart disease (angiographic disease status)\n",
    "    - Value 0: < 50% diameter narrowing\n",
    "    - Value 1: > 50% diameter narrowing\n",
    "    (in any major vessel: attributes 59 through 68 are vessels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(\"../data/heart-disease/processed.cleveland.data\", delimiter=\",\",\n",
    "                       names=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "                              \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"])\n",
    "heart_df = heart_df.rename(columns={\"cp\":\"chest_pain\",\n",
    "                                    \"thalach\":\"max_heart_rate\",\n",
    "                                    \"oldpeak\":\"st_dep_induced\",\n",
    "                                    \"ca\":\"num_maj_ves\"})\n",
    "\n",
    "heart_df['num_bin'] = heart_df['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "heart_df = heart_df.replace('?', np.nan)\n",
    "heart_df = heart_df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "heart_df[['num_maj_ves', 'thal']] = heart_df[['num_maj_ves', 'thal']].astype('float')\n",
    "heart_df= heart_df.reset_index(drop=True)\n",
    "\n",
    "heart_df.drop(\"num\", inplace=True, axis=1)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heart_df_plot = heart_df.copy()\n",
    "heart_df_plot[[\"sex\", \"chest_pain\", \"fbs\",\n",
    "               \"restecg\", \"exang\", \"slope\",\n",
    "               \"num_maj_ves\", \"thal\", \"num_bin\"]] = heart_df_plot[[\"sex\", \"chest_pain\", \"fbs\", \"restecg\",\n",
    "                                                                   \"exang\", \"slope\", \"num_maj_ves\", \"thal\", \n",
    "                                                                   \"num_bin\"]\n",
    "                                                                 ].apply(lambda x: x.astype('category'))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=7, ncols=2, figsize=(10, 25))\n",
    "ax = ax.reshape(-1)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.5)\n",
    "\n",
    "for i, col in enumerate(heart_df_plot.columns):\n",
    "    if heart_df_plot[col].dtype.name == \"category\":\n",
    "        sns.countplot(x=col, data=heart_df_plot, ax=ax[i])\n",
    "        ax[i].set_ylabel(\"count\")\n",
    "    else:\n",
    "        sns.kdeplot(heart_df_plot[col], ax=ax[i])\n",
    "    ax[i].set_xlabel(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standardizing dataset\n",
    "standardizing_heart_df = heart_df.copy()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Checking out the scattered matrix\n",
    "standardizing_heart_df.iloc[:, :-1] = scaler.fit_transform(heart_df.drop(['num_bin'], axis=1))\n",
    "pd.plotting.scatter_matrix(standardizing_heart_df, alpha=0.3, figsize=(15,15), diagonal='kde');\n",
    "\n",
    "# Checking our the heatmap of correlations\n",
    "plt.figure(figsize=(15, 15))\n",
    "correlation = standardizing_heart_df.corr(min_periods=10)\n",
    "heatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        self.heart_df = df\n",
    "        self.number_samples = len(df)\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.number_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.train:\n",
    "            idx = torch.LongTensor([idx])\n",
    "        features = self.heart_df.iloc[idx.item(), :-1]\n",
    "        binary_class = self.heart_df.iloc[idx.item(), -1]\n",
    "        return torch.FloatTensor(features.values), binary_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_sample_number = int(train_size*len(heart_df))\n",
    "\n",
    "weight_one = 1 - sum(heart_df.iloc[:,-1]) / len(heart_df)\n",
    "weight = [weight_one if c == 1 else 1 - weight_one for c in heart_df.iloc[:,-1] ]\n",
    "\n",
    "total_dataset = HeartDataset(heart_df)\n",
    "sampler = WeightedRandomSampler(weight, len(total_dataset), replacement=False)\n",
    "batch_sampler = BatchSampler(sampler, train_sample_number, drop_last=False)\n",
    "\n",
    "train_set, test_set = ([i.item() for i in cl] for cl in batch_sampler)\n",
    "train_df = heart_df.iloc[train_set]\n",
    "test_df = heart_df.iloc[test_set]\n",
    "print(\"training set: {} samples, {:.2f}% of which belong to the first class\".format(len(train_df), train_df.num_bin.value_counts().values[0] / len(train_df) * 100))\n",
    "print(\"testing set: {} samples, {:.2f}% of which belong to the first class\".format(len(test_df), test_df.num_bin.value_counts().values[0] / len(test_df) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "# DO NOT STANDARDIZE FOR RFC, NO USE FOR IT, PROBLEMS WITH ANALYZING RESULTS IF WE DO\n",
    "Training with 5-fold cross validation to find the best parameters, and computing the accuracy, precision/recall/f1score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL CELL FOR RFC START\n",
    "\n",
    "# The goal is to predict if a person has a heart disease or not.\n",
    "# Thus we have decided to try a random forest classifier\n",
    "# Has we want to do a kind of diagnosis thus we want to optimize the recall\n",
    "# Because it is more dangerous to miss a heart disease than to over diagnosis\n",
    "train_features, train_targets = train_df.iloc[:,:-1], train_df.iloc[:,-1]\n",
    "test_features, test_targets = test_df.iloc[:,:-1], test_df.iloc[:,-1]\n",
    "\n",
    "# classic random forest classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# the parameters we want to test in order to find the best combinations\n",
    "parameters = {'n_estimators': list(range(5,40,5)), 'max_features':list(range(2,10)), 'max_depth': list(range(2,10))}\n",
    "# the score used to optimize the classifier (recall here)\n",
    "recall_score = metrics.make_scorer(metrics.recall_score)\n",
    "# then we can make the grid search cross validation (5-fold)\n",
    "grid_search = GridSearchCV(rfc, parameters, scoring=recall_score, cv=5)\n",
    "# we can fit the grid search object with our training set\n",
    "grid_search_fit = grid_search.fit(train_features, train_targets)\n",
    "# then we can deduce the best classifier (the best parameters combination)\n",
    "best_rfc = grid_search_fit.best_estimator_\n",
    "# and we make the predictions for our testing set\n",
    "best_predict = best_rfc.predict(test_features)\n",
    "\n",
    "print(\"Best parameters : \", grid_fit.best_params_)\n",
    "# accuracy = tp + tn / total, pas bon si desequilibre des labels \n",
    "# (le classifier peut donner toujours le même label)\n",
    "print(\"\\naccuracy :\", metrics.accuracy_score(test_targets, best_predict))\n",
    "# recall = tp / tp + fn (total de vrai malade du test set) => très important ici,\n",
    "# car on veut trouver le plus de malade possible (plus grave de rater un malade, que d'en diagnostiquer trop)\n",
    "print(\"recall :\", metrics.recall_score(test_targets, best_predict))\n",
    "# precision = tp / tp + fp (total de malade que le classifier a trouvé) => moins important ici,\n",
    "# car determine combien de diagnostiqués malades, sont vraiment malades \n",
    "print(\"precision :\", metrics.precision_score(test_targets, best_predict))\n",
    "# f1 score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"f1 score :\", metrics.f1_score(test_targets, best_predict), \"\\n\")\n",
    "\n",
    "# importance of each features\n",
    "for i, feat in enumerate(best_rfc.feature_importances_):\n",
    "    print(\"{} : {}\".format(list(heart_df)[i], feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n",
    "\n",
    "# Visualize the decision tree\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract single tree of our best random forest classifier\n",
    "estimator = best_rfc.estimators_[0]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = list(train_df.iloc[:, :-1]),\n",
    "                class_names = iris.target_names,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "# STANDARDIZE TRAINING SET AND TESTING SET THE SAME WAY TO BE SURE WE GET THE SAME THING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a validation set\n",
    "train_nn_size = 0.8\n",
    "train_nn_sample_number = int(train_nn_size*len(train_df))\n",
    "\n",
    "weight_one = 1 - sum(train_df.iloc[:,-1]) / len(train_df)\n",
    "weight = [weight_one if c == 1 else 1 - weight_one for c in train_df.iloc[:,-1] ]\n",
    "\n",
    "# Standardizing training set\n",
    "standardized_nn_train_df = train_df.copy()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(train_df.drop(['num_bin'], axis=1))\n",
    "standardized_nn_train_df.iloc[:, :-1] = scaler.transform(train_df.drop(['num_bin'], axis=1))\n",
    "\n",
    "train_nn_set = HeartDataset(standardized_nn_train_df)\n",
    "sampler = WeightedRandomSampler(weight, len(train_nn_set), replacement=False)\n",
    "batch_sampler = BatchSampler(sampler, train_nn_sample_number, drop_last=False)\n",
    "\n",
    "train_set, valid_set = ([i.item() for i in cl] for cl in batch_sampler)\n",
    "train_nn_df = train_df.iloc[train_set]\n",
    "valid_df = train_df.iloc[valid_set]\n",
    "print(\"training set: {} samples, {:.2f}% of which belong to the first class\".format(len(train_nn_df), train_nn_df.num_bin.value_counts().values[0] / len(train_nn_df) * 100))\n",
    "print(\"validation set: {} samples, {:.2f}% of which belong to the first class\".format(len(valid_df), valid_df.num_bin.value_counts().values[0] / len(valid_df) * 100))\n",
    "print(\"testing set: {} samples, {:.2f}% of which belong to the first class\".format(len(test_df), test_df.num_bin.value_counts().values[0] / len(test_df) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset = HeartDataset(train_nn_df)\n",
    "valid_dataset = HeartDataset(valid_df, train=False)\n",
    "\n",
    "# Do not forget to apply the same preprocessing to the testing set\n",
    "standardizing_test_df = test_df.copy()\n",
    "standardizing_test_df.iloc[:, :-1] = scaler.transform(test_df.drop(['num_bin'], axis=1))\n",
    "test_dataset = HeartDataset(standardizing_test_df, train=False)\n",
    "\n",
    "weight_one = 1 - sum(train_nn_df.iloc[:,-1]) / len(train_nn_df)\n",
    "weight = [weight_one if c == 1 else 1 - weight_one for c in train_nn_df.iloc[:,-1] ]\n",
    "\n",
    "sampler = WeightedRandomSampler(weight, len(train_dataset))\n",
    "b_sampler = BatchSampler(sampler, batch_size, True)\n",
    "\n",
    "train_dataset_loader = DataLoader(dataset=train_dataset,  num_workers=0, batch_sampler=b_sampler )\n",
    "valid_dataset_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size)\n",
    "test_dataset_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset_loader, valid_dataset_loader, nb_epochs):\n",
    "    \"\"\"\n",
    "    Will train the model and retain the training and validation losses at each epoch\n",
    "    \"\"\"\n",
    "    training_losses, validation_losses = [], []\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for e in range(nb_epochs):\n",
    "        train_sum_loss = 0\n",
    "        valid_sum_loss = 0\n",
    "        \n",
    "        for feat, c in train_dataset_loader:\n",
    "            output = model(feat)\n",
    "            loss = criterion(output, c)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_sum_loss += loss.item()\n",
    "            \n",
    "        for feat, c in valid_dataset_loader:\n",
    "            output = model(feat)\n",
    "            loss = criterion(output, c)\n",
    "            valid_sum_loss += loss.item()\n",
    "            \n",
    "        if (e+1) % 100 == 0:\n",
    "            print(\"epoch: {:4d}/{:4d} | training loss: {:3.4f} | validation loss: {:3.4f}\".format(\n",
    "                e+1, nb_epochs, normalized_train_loss, normalized_valid_loss))\n",
    "            \n",
    "        if (e+1) % 5 == 0:\n",
    "            normalized_train_loss = train_sum_loss / len(train_dataset_loader)\n",
    "            normalized_valid_loss = valid_sum_loss / len(valid_dataset_loader)\n",
    "            training_losses.append(normalized_train_loss)\n",
    "            validation_losses.append(normalized_valid_loss)\n",
    "        \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Training and validation losses\")\n",
    "    plt.plot(training_losses, 'red', label='training loss')\n",
    "    plt.plot(validation_losses, 'green', label='validation loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def compute_nb_errors(model, dataset_loader):\n",
    "    model.eval()\n",
    "    nb_data_errors = 0\n",
    "    #nb_recall_errors = 0\n",
    "    #nb_pos = 0\n",
    "    \n",
    "    for feat, c in dataset_loader:\n",
    "        output = model(feat)\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        nb_data_errors += sum(abs(predicted_classes - c)).item()\n",
    "        #DO RECALL\n",
    "        #nb_recall_errors += sum(predicted_classes == 1 & c == 1).item()\n",
    "        #nb_pos += sum(c == 1).item()\n",
    "\n",
    "    accuracy = (1 - nb_data_errors/ (len(dataset_loader)*batch_size)) *100\n",
    "    #recall = nb_recall_errors / nb_pos * 100\n",
    "    return accuracy#, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop = 0.6\n",
    "model = torch.nn.Sequential(torch.nn.Linear(13,200),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(200,100),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(100,50),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(50,2))\n",
    "train_model(model, train_dataset_loader, valid_dataset_loader, 1200)\n",
    "print(\"With dropout:\", drop)\n",
    "print(\"training set accuracy: {:2.2f}%\".format(compute_nb_errors(model, train_dataset_loader)))\n",
    "print(\"validation set accuracy: {:2.2f}%\".format(compute_nb_errors(model, valid_dataset_loader)))\n",
    "print(\"testing set accuracy: {:2.2f}%\".format(compute_nb_errors(model, test_dataset_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
