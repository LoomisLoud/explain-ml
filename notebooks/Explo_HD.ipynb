{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining maching learning\n",
    "You can find in this notebook the exploratory data analysis done over the heart disease dataset, coupled with the simple machine learning algorithm used to predict the diseases.\n",
    "\n",
    "## Plan\n",
    "1. Introduction\n",
    "  1. What are we going to explain here\n",
    "    1. Explain AI/ML\n",
    "    2. Not a tutorial, un exemple concret tout public de à quoi ça pourrait servir\n",
    "    3. Essayer d'expliquer le taff d'un DS pour les gens qu'on connait et ceux interessés en general\n",
    "  2. Why do we need to talk about AI in general\n",
    "    1. Future\n",
    "    2. Generic term needs to be addressed\n",
    "    3. See the good side\n",
    "2. What is a Data Scientist\n",
    "  1. Work with data\n",
    "  2. Data Analysis\n",
    "  2. create models\n",
    "  3. communicate\n",
    "3. Example dataset\n",
    "  1. Explaining the dataset and the goal\n",
    "  2. A few statistics/plots\n",
    "  3. Predicting the heart disease\n",
    "  4. Explaining results\n",
    "4. Communicating results to business/boss\n",
    "  1. Presenting the example's results\n",
    "  2. Another level of abstraction ?\n",
    "5. (OPT) Cleaning codebase/Explaining why we would need another language\n",
    "6. Conclusions, pointing to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import SequentialSampler, DataLoader, WeightedRandomSampler, BatchSampler\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset fields description\n",
    "1. age: age in years\n",
    "2. sex: sex (1 = male; 0 = female)\n",
    "3. cp: chest pain type\n",
    "    - Value 1: typical angina\n",
    "    - Value 2: atypical angina\n",
    "    - Value 3: non-anginal pain\n",
    "    - Value 4: asymptomatic\n",
    "4. trestbps: resting blood pressure (in mm Hg on admission to the \n",
    "    hospital)\n",
    "5. chol: serum cholestoral in mg/dl\n",
    "6. fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n",
    "7. restecg: resting electrocardiographic results\n",
    "    - Value 0: normal\n",
    "    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "    - Value 1: upsloping\n",
    "    - Value 2: flat\n",
    "    - Value 3: downsloping\n",
    "12. ca: number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "14. num: diagnosis of heart disease (angiographic disease status)\n",
    "    - Value 0: < 50% diameter narrowing\n",
    "    - Value 1: > 50% diameter narrowing\n",
    "    (in any major vessel: attributes 59 through 68 are vessels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(\"../data/heart-disease/processed.cleveland.data\", delimiter=\",\",\n",
    "                       names=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "                              \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"])\n",
    "heart_df = heart_df.rename(columns={\"cp\":\"chest_pain\",\n",
    "                                    \"thalach\":\"max_heart_rate\",\n",
    "                                    \"oldpeak\":\"st_dep_induced\",\n",
    "                                    \"ca\":\"num_maj_ves\"})\n",
    "\n",
    "heart_df['num_bin'] = heart_df['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "heart_df = heart_df.replace('?', np.nan)\n",
    "heart_df = heart_df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "heart_df[['num_maj_ves', 'thal']] = heart_df[['num_maj_ves', 'thal']].astype('float')\n",
    "heart_df= heart_df.reset_index(drop=True)\n",
    "\n",
    "heart_df.drop(\"num\", inplace=True, axis=1)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heart_df_plot = heart_df.copy()\n",
    "heart_df_plot[[\"sex\", \"chest_pain\", \"fbs\",\n",
    "               \"restecg\", \"exang\", \"slope\",\n",
    "               \"num_maj_ves\", \"thal\", \"num_bin\"]] = heart_df_plot[[\"sex\", \"chest_pain\", \"fbs\", \"restecg\",\n",
    "                                                                   \"exang\", \"slope\", \"num_maj_ves\", \"thal\", \n",
    "                                                                   \"num_bin\"]\n",
    "                                                                 ].apply(lambda x: x.astype('category'))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=7, ncols=2, figsize=(10, 25))\n",
    "ax = ax.reshape(-1)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.5)\n",
    "\n",
    "for i, col in enumerate(heart_df_plot.columns):\n",
    "    if heart_df_plot[col].dtype.name == \"category\":\n",
    "        sns.countplot(x=col, data=heart_df_plot, ax=ax[i])\n",
    "        ax[i].set_ylabel(\"count\")\n",
    "    else:\n",
    "        sns.kdeplot(heart_df_plot[col], ax=ax[i])\n",
    "    ax[i].set_xlabel(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standardizing dataset\n",
    "standardizing_heart_df = heart_df.copy()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Checking out the scattered matrix\n",
    "standardizing_heart_df.iloc[:, :-1] = scaler.fit_transform(heart_df.drop(['num_bin'], axis=1))\n",
    "pd.plotting.scatter_matrix(standardizing_heart_df, alpha=0.3, figsize=(15,15), diagonal='kde');\n",
    "\n",
    "# Checking our the heatmap of correlations\n",
    "plt.figure(figsize=(15, 15))\n",
    "correlation = standardizing_heart_df.corr(min_periods=10)\n",
    "heatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        self.heart_df = df\n",
    "        self.number_samples = len(df)\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.number_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.train:\n",
    "            idx = torch.LongTensor([idx])\n",
    "        features = self.heart_df.iloc[idx.item(), :-1]\n",
    "        binary_class = self.heart_df.iloc[idx.item(), -1]\n",
    "        return torch.FloatTensor(features.values), binary_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.9\n",
    "train_sample_number = int(train_size*len(heart_df))\n",
    "\n",
    "weight_one = 1 - sum(heart_df.iloc[:,-1]) / len(heart_df)\n",
    "weight = [weight_one if c == 1 else 1 - weight_one for c in heart_df.iloc[:,-1] ]\n",
    "\n",
    "total_dataset = HeartDataset(heart_df)\n",
    "sampler = WeightedRandomSampler(weight, len(total_dataset), replacement=False)\n",
    "batch_sampler = BatchSampler(sampler, train_sample_number, drop_last=False)\n",
    "\n",
    "train_set, test_set = ([i.item() for i in cl] for cl in batch_sampler)\n",
    "train_df = heart_df.iloc[train_set]\n",
    "test_df = heart_df.iloc[test_set]\n",
    "print(\"training set: {} samples, {:.2f}% of which belong to the first class\".format(len(train_df), train_df.num_bin.value_counts().values[0] / len(train_df) * 100))\n",
    "print(\"testing set: {} samples, {:.2f}% of which belong to the first class\".format(len(test_df), test_df.num_bin.value_counts().values[0] / len(test_df) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "# DO NOT STANDARDIZE FOR RFC, NO USE FOR IT, PROBLEMS WITH ANALYZING RESULTS IF WE DO\n",
    "Training with 5-fold cross validation to find the best parameters, and computing the accuracy, precision/recall/f1score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL CELL FOR RFC START\n",
    "\n",
    "# The goal is to predict if a person has a heart disease or not.\n",
    "# Thus we have decided to try a random forest classifier\n",
    "# Has we want to do a kind of diagnosis thus we want to optimize the recall\n",
    "# Because it is more dangerous to miss a heart disease than to over diagnosis\n",
    "train_features, train_targets = train_df.iloc[:,:-1], train_df.iloc[:,-1]\n",
    "test_features, test_targets = test_df.iloc[:,:-1], test_df.iloc[:,-1]\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [5, 10, 20, 30], 'max_features':[3,4,5,6, None], 'max_depth': [4,5,6,7, None]}\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv=5)\n",
    "grid_fit = grid_obj.fit(train_features, train_targets)\n",
    "best_clf = grid_fit.best_estimator_\n",
    "best_predictions = best_clf.predict(test_features)\n",
    "print(\"Best parameters : \", grid_fit.best_params_)\n",
    "# accuracy = tp + tn / total, pas bon si desequilibre des labels \n",
    "# (le classifier peut donner toujours le même label)\n",
    "print(\"\\nFinal accuracy score on the testing data: {:.4f}\".format(metrics.accuracy_score(test_targets, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(metrics.fbeta_score(test_targets, best_predictions, beta=0.5,  average=\"micro\")))\n",
    "# recall = tp / tp + fn (total de vrai malade du test set) => très important ici,\n",
    "# car on veut trouver le plus de malade possible (plus grave de rater un malade, que d'en diagnostiquer trop)\n",
    "print(\"recall :\", metrics.recall_score(test_targets, best_predictions))\n",
    "# precision = tp / tp + fp (total de malade que le classifier a trouvé) => moins important ici,\n",
    "# car determine combien de diagnostiqués malades, sont vraiment malades \n",
    "print(\"precision :\", metrics.precision_score(test_targets, best_predictions))\n",
    "# f1 score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"f1 score :\", metrics.f1_score(test_targets, best_predictions))\n",
    "\n",
    "### 3 other cells draft ! to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=10)\n",
    "\n",
    "clf.fit(train_features, train_targets)\n",
    "\n",
    "for i, feat in enumerate(clf.feature_importances_):\n",
    "    print(\"{} : {}\".format(list(heart_df)[i], feat))\n",
    "    \n",
    "print(\"\\naccuracy :\", clf.score(test_features, test_targets))\n",
    "\n",
    "predictions_test = clf.predict(test_features)\n",
    "# accuracy = tp + tn / total, pas bon si desequilibre des labels \n",
    "# (le classifier peut donner toujours le même label)\n",
    "print(\"\\naccuracy :\", metrics.accuracy_score(test_targets, predictions_test))\n",
    "# recall = tp / tp + fn (total de vrai malade du test set) => très important ici,\n",
    "# car on veut trouver le plus de malade possible (plus grave de rater un malade, que d'en diagnostiquer trop)\n",
    "print(\"recall :\", metrics.recall_score(test_targets, predictions_test))\n",
    "# precision = tp / tp + fp (total de malade que le classifier a trouvé) => moins important ici,\n",
    "# car determine combien de diagnostiqués malades, sont vraiment malades \n",
    "print(\"precision :\", metrics.precision_score(test_targets, predictions_test))\n",
    "# f1 score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"f1 score :\", metrics.f1_score(test_targets, predictions_test))\n",
    "# Confusion Matrix : 0 negatif, 1 positif => [[tn, fp], [fn, tp]]\n",
    "print(\"\\nconfusion matrix :\\n\", metrics.confusion_matrix(test_targets, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# DOES NOT WORK, I DID NOT REPLACE EVERYTHING\n",
    "# DRAFT\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "parameters = {'n_estimators': [5, 10, 20, 30], 'max_features':[3,4,5,6, None], 'max_depth': [4,5,6,7, None]}\n",
    "\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# TODO: Perform grid search on the claszsifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv=5)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"Accuracy score on testing data: {:.4f}\".format(metrics.accuracy_score(y_test, predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(metrics.fbeta_score(y_test, predictions, beta = 0.5, average=\"micro\")))\n",
    "print(\"recall :\", metrics.recall_score(y_test, predictions))\n",
    "print(\"precision :\", metrics.precision_score(y_test, predictions))\n",
    "print(\"f1 score :\", metrics.f1_score(y_test, predictions))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(grid_fit.best_params_)\n",
    "print(\"\\nFinal accuracy score on the testing data: {:.4f}\".format(metrics.accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(metrics.fbeta_score(y_test, best_predictions, beta = 0.5,  average=\"micro\")))\n",
    "print(\"recall :\", metrics.recall_score(y_test, best_predictions))\n",
    "print(\"precision :\", metrics.precision_score(y_test, best_predictions))\n",
    "print(\"f1 score :\", metrics.f1_score(y_test, best_predictions))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Model (can also use single decision tree)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=10)\n",
    "\n",
    "# Train\n",
    "model.fit(train_features, train_targets)\n",
    "# Extract single tree\n",
    "estimator = model.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = list(train_df.iloc[:, :-1]),\n",
    "                class_names = iris.target_names,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "# STANDARDIZE TRAINING SET AND TESTING SET THE SAME WAY TO BE SURE WE GET THE SAME THING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a validation set\n",
    "train_nn_size = 0.9\n",
    "fold_size = 0.2\n",
    "train_nn_sample_number = int(train_nn_size*len(train_df))\n",
    "\n",
    "print(train_nn_sample_number)\n",
    "\n",
    "weight_one = 1 - sum(train_df.iloc[:,-1]) / len(train_df)\n",
    "weight = [weight_one if c == 1 else 1 - weight_one for c in train_df.iloc[:,-1] ]\n",
    "\n",
    "# Standardizing training set\n",
    "standardized_nn_train_df = train_df.copy()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(train_df.drop(['num_bin'], axis=1))\n",
    "standardized_nn_train_df.iloc[:, :-1] = scaler.transform(train_df.drop(['num_bin'], axis=1))\n",
    "\n",
    "train_nn_set = HeartDataset(standardized_nn_train_df)\n",
    "sampler = WeightedRandomSampler(weight, len(train_nn_set), replacement=False)\n",
    "batch_sampler = BatchSampler(sampler, train_nn_sample_number, drop_last=False)\n",
    "\n",
    "train_set, valid_set = ([i.item() for i in cl] for cl in batch_sampler)\n",
    "train_nn_df = train_df.iloc[train_set]\n",
    "valid_df = train_df.iloc[valid_set]\n",
    "print(\"training set: {} samples, {:.2f}% of which belong to the first class\".format(len(train_nn_df), train_nn_df.num_bin.value_counts().values[0] / len(train_nn_df) * 100))\n",
    "print(\"validation set: {} samples, {:.2f}% of which belong to the first class\".format(len(valid_df), valid_df.num_bin.value_counts().values[0] / len(valid_df) * 100))\n",
    "print(\"testing set: {} samples, {:.2f}% of which belong to the first class\".format(len(test_df), test_df.num_bin.value_counts().values[0] / len(test_df) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_loader(train_df, valid_df, test_df, batch_size=8):\n",
    "    train_dataset = HeartDataset(train_nn_df)\n",
    "    valid_dataset = HeartDataset(valid_df, train=False)\n",
    "\n",
    "    # Do not forget to apply the same preprocessing to the testing set\n",
    "    standardized_test_df = test_df.copy()\n",
    "    standardized_test_df.iloc[:, :-1] = scaler.transform(test_df.drop(['num_bin'], axis=1))\n",
    "    test_dataset = HeartDataset(standardized_test_df, train=False)\n",
    "\n",
    "    weight_one = 1 - sum(train_nn_df.iloc[:,-1]) / len(train_nn_df)\n",
    "    weight = [weight_one if c == 1 else 1 - weight_one for c in train_nn_df.iloc[:,-1] ]\n",
    "\n",
    "    sampler = WeightedRandomSampler(weight, len(train_dataset))\n",
    "    b_sampler = BatchSampler(sampler, batch_size, True)\n",
    "\n",
    "    train_dataset_loader = DataLoader(dataset=train_dataset,  num_workers=0, batch_sampler=b_sampler )\n",
    "    valid_dataset_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size)\n",
    "    test_dataset_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataset_loader, valid_dataset_loader, test_dataset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset_loader, valid_dataset_loader, nb_epochs, regul=0):\n",
    "    \"\"\"\n",
    "    Will train the model and retain the training and validation losses at each epoch\n",
    "    \"\"\"\n",
    "    training_losses, validation_losses = [], []\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=regul)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        train_sum_loss = 0\n",
    "        valid_sum_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        for feat, c in train_dataset_loader:\n",
    "            output = model(feat)\n",
    "            loss = criterion(output, c)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_sum_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        for feat, c in valid_dataset_loader:\n",
    "            output = model(feat)\n",
    "            loss = criterion(output, c)\n",
    "            valid_sum_loss += loss.item()\n",
    "            \n",
    "        if (e+1) % 100 == 0:\n",
    "            print(\"epoch: {:4d}/{:4d} | training loss: {:3.4f} | validation loss: {:3.4f}\".format(\n",
    "                e+1, nb_epochs, normalized_train_loss, normalized_valid_loss))\n",
    "            \n",
    "        normalized_train_loss = train_sum_loss / len(train_dataset_loader)\n",
    "        normalized_valid_loss = valid_sum_loss / len(valid_dataset_loader)\n",
    "        training_losses.append(normalized_train_loss)\n",
    "        validation_losses.append(normalized_valid_loss)\n",
    "        \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Training and validation losses\")\n",
    "    plt.plot(training_losses, 'red', label='training loss')\n",
    "    plt.plot(validation_losses, 'green', label='validation loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, dataset_loader, batch_size=8):\n",
    "    # Precision and f1-score\n",
    "    model.eval()\n",
    "    nb_accuracy_errors = 0\n",
    "    nb_true_pos = 0\n",
    "    nb_pos = 0\n",
    "    nb_false_pos = 0\n",
    "    \n",
    "    for feat, c in dataset_loader:\n",
    "        output = model(feat)\n",
    "        _, predicted_classes = torch.max(output, 1)\n",
    "        nb_accuracy_errors += sum(abs(predicted_classes - c)).item()\n",
    "        # Computing recall: summing the predictions with the targets\n",
    "        # and counting the number of \"2\"\n",
    "        nb_true_pos += ((predicted_classes + c) == 2).sum().item()\n",
    "        nb_pos += c.sum().item()\n",
    "        # Computing precision: nb of true pos divided by nb of\n",
    "        # true pos + nb false pos\n",
    "        nb_false_pos = predicted_classes.sum().item()\n",
    "\n",
    "    accuracy = (1 - nb_accuracy_errors/ (len(dataset_loader)*batch_size))\n",
    "    precision = nb_true_pos / (nb_true_pos + nb_false_pos)\n",
    "    recall = nb_true_pos / nb_pos\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cv(train_df, model, l2_penalty, epochs):\n",
    "\n",
    "\n",
    "    total_accuracy_v = 0\n",
    "    total_precision_v = 0\n",
    "    total_recall_v = 0\n",
    "\n",
    "    total_accuracy_t = 0\n",
    "    total_precision_t = 0\n",
    "    total_recall_t = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(5):\n",
    "        \n",
    "        current_model = copy.deepcopy(model)\n",
    "        fold_size = 0.2\n",
    "        train_nn_index = int(fold_size*len(train_df))\n",
    "        index_list = list(range(train_nn_index * i, train_nn_index * (i+1)))\n",
    "        valid_index = train_df.index.isin(index_list)\n",
    "        valid_df = train_df[valid_index]\n",
    "        train_nn_df = train_df[~valid_index]\n",
    "        print(\"training set: {} samples, {:.2f}% of which belong to the first class\".format(len(train_nn_df), train_nn_df.num_bin.value_counts().values[0] / len(train_nn_df) * 100))\n",
    "        print(\"validation set: {} samples, {:.2f}% of which belong to the first class\".format(len(valid_df), valid_df.num_bin.value_counts().values[0] / len(valid_df) * 100))\n",
    "\n",
    "        train_dataset_loader, valid_dataset_loader, test_dataset_loader = built_loader(train_nn_df, valid_df, test_df)\n",
    "        \n",
    "        train_model(current_model, train_dataset_loader, valid_dataset_loader, epochs, l2_penalty)\n",
    "        accuracy_valid, precision_valid, recall_valid = evaluate_model(current_model, valid_dataset_loader)\n",
    "        accuracy_train, precision_train, recall_train = evaluate_model(current_model, train_dataset_loader)\n",
    "\n",
    "        total_accuracy_v += accuracy_valid\n",
    "        total_accuracy_t += accuracy_train\n",
    "\n",
    "        total_precision_v += precision_valid\n",
    "        total_precision_t += precision_train\n",
    "\n",
    "        total_recall_v += recall_valid\n",
    "        total_recall_t += recall_train\n",
    "\n",
    "    mean_accuracy_v = total_accuracy_v / 5\n",
    "    mean_accuracy_t = total_accuracy_t / 5\n",
    "\n",
    "    mean_precision_v = total_precision_v / 5\n",
    "    mean_precision_t = total_precision_t / 5\n",
    "\n",
    "    mean_recall_v = total_recall_v / 5\n",
    "    mean_recall_t = total_recall_t / 5\n",
    "\n",
    "\n",
    "    print(\"Training set:\\n\\t- accuracy: {:2.2%}\\n\\t- precision: {:2.2%}\\n\\t- recall: {:2.2%}\\n\"\n",
    "          .format(mean_accuracy_t, mean_precision_t, mean_recall_t))\n",
    "    print(\"Validation set:\\n\\t- accuracy: {:2.2%}\\n\\t- precision: {:2.2%}\\n\\t- recall: {:2.2%}\\n\"\n",
    "          .format(mean_accuracy_v, mean_precision_v, mean_recall_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop = 0.2\n",
    "model = torch.nn.Sequential(torch.nn.Linear(13,50),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(50,100),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(100,50),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(50,2))\n",
    "cv(train_df, model, 0.03, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 1000\n",
    "model = torch.nn.Sequential(torch.nn.Linear(13,50),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(50,100),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(100,50),\n",
    "                            torch.nn.Dropout(drop),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(50,2))\n",
    "train_model(model, train_dataset_loader, valid_dataset_loader, epochs, l2_penalty)\n",
    "print(\"For regularizer:\", l2_penalty, \"dropout:\", drop, \"epochs:\", epochs)\n",
    "\n",
    "# Training set evaluation\n",
    "accuracy, precision, recall = evaluate_model(model, train_dataset_loader)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(\"Training set:\\n\\t- accuracy: {:2.2%}\\n\\t- precision: {:2.2%}\\n\\t- recall: {:2.2%}\\n\\t- f1-score: {:2.2%}\\n\"\n",
    "      .format(accuracy, precision, recall, f1_score))\n",
    "# Validation set evaluation\n",
    "accuracy, precision, recall = evaluate_model(model, valid_dataset_loader)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(\"Validation set:\\n\\t- accuracy: {:2.2%}\\n\\t- precision: {:2.2%}\\n\\t- recall: {:2.2%}\\n\\t- f1-score: {:2.2%}\\n\"\n",
    "      .format(accuracy, precision, recall, f1_score))\n",
    "# Testing set evaluation\n",
    "accuracy, precision, recall = evaluate_model(model, test_dataset_loader)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print(\"Testing set:\\n\\t- accuracy: {:2.2%}\\n\\t- precision: {:2.2%}\\n\\t- recall: {:2.2%}\\n\\t- f1-score: {:2.2%}\\n\"\n",
    "      .format(accuracy, precision, recall, f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
